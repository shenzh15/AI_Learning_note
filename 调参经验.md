# 深度学习调参经验

---

## 一、学习率大小的影响 (The Effect of Learning Rate)

学习率（Learning Rate）是深度学习训练中最重要的超参数之一，它决定了参数更新的步长。设置合适的学习率对模型能否成功收敛至关重要。

- **学习率过大**:
  - **现象**：损失函数（Loss）可能会在最小值附近剧烈震荡，甚至可能导致 Loss 不断增大，出现“发散”的情况，模型完全无法收敛。
  - **原因**：参数更新的步子迈得太大，很容易直接“跨过”最优解（最小值点），并在最优解的两侧来回跳动。

- **学习率过小**:
  - **现象**：训练过程会变得极其缓慢。在某些情况下，模型可能会陷入一个不够好的局部最小值或鞍点，难以“跳出来”去寻找更好的解。
  - **原因**：参数更新的步子太小，每次只移动一点点，导致需要非常多的迭代次数才能到达最优点。

- **合适的学习率**:
  - **现象**：损失函数能够稳定、快速地下降，最终收敛到一个较低的水平。
  - **经验方法**：最佳学习率通常在模型即将发散的临界点附近。

---

## 二、Batch Size 与学习率的缩放关系

Batch Size 和学习率是两个紧密相关的超参数。通常，当调整 Batch Size 时，也应该相应地调整学习率。

- **基本原则**：当 Batch Size 增大时，每个 step 计算的梯度会更准确，噪声更少，因为它是在更多样本上平均得到的。因此，我们可以更信任梯度的方向，并使用更大的学习率来加速收敛。

- **常见的缩放规则**:
  1. **线性缩放规则 (Linear Scaling Rule)**:
     - **规则**：当 Batch Size 乘以 $k$ 时，学习率也应该乘以 $k$。
     - **思想**：增大了 Batch Size，相当于减少了参数更新的次数。为了在相同的 epoch 数下走过相似的“距离”，需要增大每一步的步长（即学习率）。
     - **适用场景**：这个规则在很多计算机视觉任务中被证明是有效的，尤其是在使用带动量的 SGD 时。著名论文《Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour》中就应用了此规则。

  2. **平方根缩放规则 (Square Root Scaling Rule)**:
     - **规则**：当 Batch Size 乘以 $k$ 时，学习率应该乘以 $\sqrt{k}$。
     - **思想**：这个规则比线性缩放更保守。它认为梯度的方差（噪声）并不会随着 Batch Size 的增大而线性减小，因此学习率的增加幅度也应该更缓和。
     - **适用场景**：在某些情况下，特别是当模型对学习率比较敏感，或者使用 Adam 等自适应优化器时，平方根缩放可能是更安全的选择。

---

## 三、小 Batch Size 下的 Adam 参数调整

当硬件条件限制，只能使用较小的 Batch Size 时，Adam/AdamW 优化器的参数也需要进行相应调整，以保证训练的稳定性。

- **核心问题**：小 Batch Size 意味着每个 step 计算的梯度方差很大，噪声很多。这会影响 Adam 中二阶动量 $v_t$ 的估计。$v_t$ 用于自适应地调整每个参数的学习率，如果它剧烈波动，会导致训练不稳定。

- **调整建议**:
  - **增大 $\beta_2$**：将 $\beta_2$ 的值从默认的 `0.999` 调大，例如 `0.9995` 或 `0.9999`。
  - **原因**：$\beta_2$ 控制二阶动量 $v_t$ 的指数滑动平均。更大的 $\beta_2$ 意味着对历史梯度平方的“记忆”更长（$1/(1-0.9999) = 10000$ 步，远大于 $1/(1-0.999)=1000$ 步）。这使得对梯度幅度的估计更加平滑和保守，能有效抵抗小 Batch Size 带来的高方差，防止学习率的自适应调整过于激进。
  - **效果**：通过这种方式，即使在梯度噪声较大的情况下，参数更新也会更加稳定，有助于模型收敛。
